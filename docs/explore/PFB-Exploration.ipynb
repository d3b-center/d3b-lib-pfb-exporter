{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "from click.testing import CliRunner\n",
    "import yaml\n",
    "from fastavro import writer, reader, parse_schema, json_writer, json_reader\n",
    "from pprint import pprint, pformat\n",
    "\n",
    "from pfb import cli\n",
    "from pfb.cli import main as pfb\n",
    "\n",
    "def read_yaml(filepath):\n",
    "    with open(filepath, \"r\") as yaml_file:\n",
    "        return yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
    "\n",
    "def read_json(filepath, default=None):\n",
    "    if (default is not None) and (not os.path.isfile(filepath)):\n",
    "        return default\n",
    "\n",
    "    with open(filepath, 'r') as data_file:\n",
    "        return json.load(data_file)\n",
    "\n",
    "def write_json(data, filepath, **kwargs):\n",
    "    with open(filepath, 'w') as json_file:\n",
    "        kwargs = {\n",
    "            'indent': 4,\n",
    "            'sort_keys': True\n",
    "        }\n",
    "        json.dump(data, json_file, **kwargs)\n",
    "        \n",
    "def pfb_invoke(*args, **kwargs):\n",
    "    # Use CliRunner to call Click cli from python\n",
    "    runner = CliRunner()\n",
    "    result = runner.invoke(pfb, args, **kwargs)\n",
    "    try:\n",
    "        assert result.exit_code == 0, result.output\n",
    "    except AssertionError:\n",
    "        print(str(result.exc_info))\n",
    "\n",
    "    return result\n",
    "\n",
    "def print_avro(input_avro, output_json=None):\n",
    "    with open(input_avro, 'rb') as fo:\n",
    "        output = {}\n",
    "        print(f'Avro file {input_avro} schema:')\n",
    "        schema = reader(fo).metadata\n",
    "        schema['avro.schema'] = json.loads(schema['avro.schema'])\n",
    "        output.update(schema)\n",
    "        fo.seek(0)\n",
    "        print(f'\\nAvro file {input_avro} data:')    \n",
    "        output['data'] = [record for record in reader(fo)]\n",
    "        pprint(output)\n",
    "        if output_json:\n",
    "            write_json(output, output_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "[\n  \"kidsfirst.FileSchema.object.kidsfirst.specimen.analyte_type is <0> of type <class 'int'> expected null\",\n  \"kidsfirst.FileSchema.object.kidsfirst.specimen.analyte_type is <0> of type <class 'int'> expected string\"\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-be58782383b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# Write\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'out.avro'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mfastavro/_write.pyx\u001b[0m in \u001b[0;36mfastavro._write.writer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfastavro/_write.pyx\u001b[0m in \u001b[0;36mfastavro._write.Writer.write\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfastavro/_validation.pyx\u001b[0m in \u001b[0;36mfastavro._validation.validate\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfastavro/_validation.pyx\u001b[0m in \u001b[0;36mfastavro._validation.validate\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfastavro/_validation.pyx\u001b[0m in \u001b[0;36mfastavro._validation.validate_record\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfastavro/_validation.pyx\u001b[0m in \u001b[0;36mfastavro._validation.validate\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfastavro/_validation.pyx\u001b[0m in \u001b[0;36mfastavro._validation.validate_union\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfastavro/_validation.pyx\u001b[0m in \u001b[0;36mfastavro._validation.validate\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfastavro/_validation.pyx\u001b[0m in \u001b[0;36mfastavro._validation.validate_record\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfastavro/_validation.pyx\u001b[0m in \u001b[0;36mfastavro._validation.validate\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfastavro/_validation.pyx\u001b[0m in \u001b[0;36mfastavro._validation.validate_union\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: [\n  \"kidsfirst.FileSchema.object.kidsfirst.specimen.analyte_type is <0> of type <class 'int'> expected null\",\n  \"kidsfirst.FileSchema.object.kidsfirst.specimen.analyte_type is <0> of type <class 'int'> expected string\"\n]"
     ]
    }
   ],
   "source": [
    "# Output avro filepath\n",
    "# data_file = 'data/kf-vanilla.avro'\n",
    "\n",
    "from fastavro import writer, reader, parse_schema\n",
    "from pprint import pprint\n",
    "\n",
    "# Avro schema describing data that will go into avro file\n",
    "schema = {\n",
    "     \"namespace\": \"kidsfirst\",\n",
    "     \"type\": \"record\",\n",
    "     \"name\": \"FileSchema\",\n",
    "     \"fields\": [\n",
    "         {\n",
    "             'name': 'object',\n",
    "             'type': [\n",
    "                {\n",
    "                    'fields': [\n",
    "                         {\"name\": \"external_id\", \"type\": [\"null\", \"string\"], 'default': None},\n",
    "                         {\"name\": \"gender\",  \"type\": [\"null\", \"string\"], 'default': None},\n",
    "                         {\"name\": \"race\",  \"type\": [\"null\", \"string\"], 'default': None},\n",
    "                         {\"name\": \"age\",  \"type\": [\"null\", \"int\"], 'default': None}\n",
    "                     ],\n",
    "                     'type': 'record',\n",
    "                     'name': 'patient'\n",
    "                },\n",
    "                {\n",
    "                    'fields': [\n",
    "                        {\"name\": \"external_id\", \"type\": [\"null\", \"string\"], 'default': None},\n",
    "                        {\"name\": \"analyte_type\",  \"type\": [\"null\", \"string\"], 'default': None},\n",
    "                        {\"name\": \"composition\",  \"type\": [\"null\", \"string\"], 'default': None},\n",
    "                        {\n",
    "                            \"name\": \"duo_ids\", \n",
    "                            \"type\": ['null', {\n",
    "                                 'type': 'array',\n",
    "                                 'items': 'string'\n",
    "                             }]\n",
    "                        }\n",
    "                     ],\n",
    "                     'type': 'record',\n",
    "                     'name': 'specimen'\n",
    "                } \n",
    "             ]\n",
    "         }\n",
    "     ]\n",
    "}\n",
    "# write_json(schema, 'data/kf-vanilla-avro-schema.json')\n",
    "\n",
    "# Create some data \n",
    "records = [\n",
    "    {'object': (\"kidsfirst.patient\", {\"external_id\": \"Patient1\", \"gender\": \"female\"})},\n",
    "    {'object': (\"kidsfirst.specimen\", {\"external_id\": \"Specimen1\", \"analyte_type\": 0})},\n",
    "]\n",
    "\n",
    "# Write \n",
    "with open('out.avro', 'wb') as out:\n",
    "    writer(out, parse_schema(schema), records, validator=True)\n",
    "    \n",
    "\n",
    "# Read\n",
    "with open('out.avro', 'rb') as fo:\n",
    "    for record in reader(fo, return_record_name=True):\n",
    "        pprint(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PFB Avro - Suitable for relational data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test data using gen3 data simulator\n",
    "# Requires the gen3 data dictionary to be stored on s3\n",
    "data_dir = 'data/simulated/'\n",
    "gen3_dd = 'data/kf-gen3-datadict.json'\n",
    "schema_avro = 'data/kf-pfb-schema.avro'\n",
    "output_avro = 'data/kf-pfb.avro'\n",
    "program = 'kidsfirst'\n",
    "project = 'drc'\n",
    "\n",
    "# Execute if you don't have any test data yet\n",
    "# !data-simulator simulate --url https://s3.amazonaws.com/singhn4-data-dict-bucket/kf-gen3-datadict.json --path data/simulated --program kidsfirst --project drc\n",
    "# !ls -l data/simulated   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create schema avro file from gen3 data dict\n",
    "kf_gen3_dd = read_yaml(gen3_dd)\n",
    "\n",
    "print('Writing PFB Schema')\n",
    "result = pfb_invoke('from', '-o', schema_avro, 'dict', gen3_dd)\n",
    "\n",
    "print('************ Display the PFB with relational model only *********')\n",
    "print_avro(schema_avro)\n",
    "\n",
    "# Write the test data to the output avro file\n",
    "print('Writing data to PFB file')\n",
    "result = pfb_invoke('from', '-o', output_avro, 'json',\n",
    "           '-s', schema_avro, \n",
    "           '--program', program,\n",
    "          '--project', project,\n",
    "          data_dir)\n",
    "\n",
    "print('\\n************ Display the PFB with the data and relational model *********')\n",
    "print_avro(output_avro, output_json='data/kf-pfb.json')\n",
    "\n",
    "print('\\n************ PFB CLI Specific Functions ************')\n",
    "# Show the avro schema in pfb file\n",
    "print('-- pfb show schema -- ')\n",
    "result = pfb_invoke('show', '-i', output_avro, 'schema')\n",
    "pprint(json.loads(result.output))\n",
    "\n",
    "# Read the data back out from the pfb file\n",
    "print('-- pfb show nodes -- ')\n",
    "result = pfb_invoke('show', '-i', output_avro, 'nodes')\n",
    "print(result.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from itertools import chain\n",
    "\n",
    "from fastavro import writer, reader, parse_schema\n",
    "from pprint import pprint, pformat\n",
    "\n",
    "\n",
    "# test_schema_file = '../../tests/data/test_schema0.json'\n",
    "# test_schema_file = '../../tests/data/test_schema.json'\n",
    "test_schema_file = '../../tests/data/pfb_export/pfb_schema.json'\n",
    "\n",
    "pfb_file = '../../tests/data/test_pfb.avro'\n",
    "metadata_file = '../../tests/data/pfb_export/metadata.json'\n",
    "\n",
    "with open(test_schema_file) as json_file:\n",
    "    test_schema = json.load(json_file)\n",
    "\n",
    "with open(metadata_file) as json_file:\n",
    "    metadata = json.load(json_file)\n",
    "\n",
    "records = [\n",
    "    metadata,\n",
    "    {'id': 'BS_082CXWXG',\n",
    "     'name': 'biospecimen',\n",
    "     'namespace': 'pfb.biospecimen',\n",
    "     'object': {'age_at_event_days': None,\n",
    "                'analyte_type': 'DNA',\n",
    "                'composition': 'Blood',\n",
    "                'concentration_mg_per_ml': None,\n",
    "                'consent_type': 'DS-MUS-SKEL-IRB',\n",
    "                'created_at': '2018-09-04T14:43:03.277375+00:00',\n",
    "                'dbgap_consent_code': 'phs001410.c2',\n",
    "                'duo_ids': None,\n",
    "                'external_aliquot_id': '6355001',\n",
    "                'external_sample_id': '6355001',\n",
    "                'kf_id': 'BS_082CXWXG',\n",
    "                'method_of_sample_procurement': None,\n",
    "                'modified_at': '2018-10-23T04:39:39.365667+00:00',\n",
    "                'ncit_id_anatomical_site': None,\n",
    "                'ncit_id_tissue_type': 'NCIT:C14165',\n",
    "                'participant_id': 'PT_VVP2BNC9',\n",
    "                'sequencing_center_id': 'SC_X1N69WJM',\n",
    "                'shipment_date': None,\n",
    "                'shipment_origin': None,\n",
    "                'source_text_anatomical_site': None,\n",
    "                'source_text_tissue_type': 'Normal',\n",
    "                'source_text_tumor_descriptor': None,\n",
    "                'spatial_descriptor': None,\n",
    "                'uberon_id_anatomical_site': None,\n",
    "                'visible': True,\n",
    "                'volume_ul': None},\n",
    "     'relations': []\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def yield_metadata():\n",
    "    yield metadata\n",
    "\n",
    "def yield_row_entities():\n",
    "    for ent in records:\n",
    "        yield ent\n",
    "\n",
    "def yield_entities():\n",
    "    return chain(yield_metadata(), yield_row_entities())\n",
    "\n",
    "if os.path.isfile(pfb_file):\n",
    "    os.remove(pfb_file)\n",
    "\n",
    "with open(pfb_file, 'a+b') as avro_file:\n",
    "    for ent in yield_entities():\n",
    "        writer(avro_file, test_schema, [ent], validator=True)\n",
    "\n",
    "with open(pfb_file, 'rb') as fo:\n",
    "    avro_reader = reader(fo)\n",
    "    for record in avro_reader:\n",
    "        pprint(record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/singhn4/Projects/kids_first/kf-ingest-packages/data'\n",
    "output_dir = '../../tests/data/input'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "193px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
